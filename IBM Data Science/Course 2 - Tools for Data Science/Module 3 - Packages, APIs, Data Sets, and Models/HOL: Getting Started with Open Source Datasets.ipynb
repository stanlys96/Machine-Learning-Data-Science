{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41611638",
   "metadata": {},
   "source": [
    "Hands on Lab: Getting Started with Open Source Datasets and Deep Learning Models\n",
    "\n",
    "In this lab, you will explore how to access and utilize open source datasets for machine learning projects. You will also get hands-on experience with pre-trained deep learning models to accelerate your development process.\n",
    "\n",
    "Objective of Exercise 1:\n",
    "\n",
    "- Explore the open data sets.\n",
    "\n",
    "Objective of Exercise 2:\n",
    "\n",
    "- Find ready-to-use deep learning models on the Model Asset Exchange.\n",
    "- Explore the deep learning model trained to detect objects in an image.\n",
    "\n",
    "It will take you approximately 15 minutes to complete the lab. Only a web browser is required to complete the tasks.\n",
    "\n",
    "Exercise 1: Explore open source datasets\n",
    "\n",
    "Most datasets are complemented by Python notebooks that you can use to explore, pre-process, and analyze the data. Here we will be exploring a public weather dataset taken from kaggle LINK\n",
    "\n",
    "Step-by-Step Instructions\n",
    "\n",
    "1. Click [HERE](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7B7CdrStANaslFDaQPTbOw/Noaadataset.html) to view the dataset.\n",
    "\n",
    "2. Once the page loads, click the Dataset Preview tab at the top of the page.\n",
    "\n",
    "This will allow you to:\n",
    "\n",
    "- View a sample of the data directly in your browser.\n",
    "\n",
    "- Understand the structure and fields of the dataset.\n",
    "\n",
    "- Explore metadata and glossary terms associated with the dataset.\n",
    "\n",
    "![NOAA](./img/Noaa.png)\n",
    "\n",
    "3. Next, click the Notebook Preview tab.\n",
    "\n",
    "- This opens a sample Python notebook associated with the dataset.\n",
    "- The notebook walks through:\n",
    "    - Cleaning the weather data.\n",
    "    - Performing exploratory analysis.\n",
    "    - Building predictive models to help airports better manage flight schedules.\n",
    "\n",
    "![NOAA2](./img/Noaa2.png)\n",
    "\n",
    "This concludes Exercise 1 of this lab, which introduced you to the open source dataset. You may proceed to Exercise 2.\n",
    "\n",
    "4. [Optional] If you are comfortable using Jupyter Notebooks, you can download and run the notebook locally by clicking\n",
    "HERE\n",
    "\n",
    "Exercise 2 - Explore deep learning models\n",
    "\n",
    "The Model Asset Exchange is a curated repository of Open Source deep learning models for a variety of domains, such as text, image, audio, and video processing.\n",
    "\n",
    "For more details, please visit - [https://github.com/CODAIT/max-central-repo](https://github.com/CODAIT/max-central-repo) webpage.\n",
    "\n",
    "The curated list includes deployable models, which you can run as a microservice locally or in the cloud on Docker or Kubernetes, and trainable models where you can use your own data to train the models. Some of the models are already built for you to test. Let's test one of the models.\n",
    "\n",
    "In this exercise, explore the Object Detector model hosted on CodePen platform. This model recognizes the objects present in an image. The model consists of a deep convolutional net base model for image feature extraction, together with additional convolutional layers specialized for the task of object detection, trained on the COCO data set. The input to the model is an image, and the output are extracted objects from the image, appropriately labeled.\n",
    "\n",
    "CodePen is a social development environment. At its heart, it allows to write code in the browser and see the results of it as you build. It is a useful and liberating online code editor for developers of any skill and is particularly empowering for people learning to code.\n",
    "\n",
    "1. Navigate to [CodePen](https://codepen.io/collection/DzdpJM/#) webpage.\n",
    "\n",
    "2. Select MAX TFJS models as shown in the screenshot below. Here the Image Segmenter, divides an image into regions or categories that correspond to different objects or parts of objects. Every pixel in an image is allocated to one of a number of these categories.\n",
    "\n",
    "![CodePen](./img/selectmodel.png)\n",
    "\n",
    "3. Click on Select Image and upload an image. You may choose images with a person, dog, cat, truck, car, and so on, which are labels the model has been trained on.\n",
    "\n",
    "![SelectImage](./img/selectimage.png)\n",
    "\n",
    "All the labels the model is trained on\n",
    "1. person\n",
    "2. bicycle\n",
    "3. car\n",
    "4. motorcycle\n",
    "5. airplane\n",
    "6. bus\n",
    "7. train\n",
    "8. truck\n",
    "9. boat\n",
    "10. traffic light\n",
    "11. fire hydrant\n",
    "12. stop sign\n",
    "13. parking meter\n",
    "14. bench\n",
    "15. bird\n",
    "16. cat\n",
    "17. dog\n",
    "18. horse\n",
    "19. sheep\n",
    "20. cow\n",
    "21. elephant\n",
    "22. bear\n",
    "23. zebra\n",
    "24. giraffe\n",
    "25. backpack\n",
    "26. umbrella\n",
    "27. handbag\n",
    "28. tie\n",
    "29. suitcase\n",
    "30. frisbee\n",
    "31. skis\n",
    "32. snowboard\n",
    "33. sports ball\n",
    "34. kite\n",
    "35. baseball bat\n",
    "36. baseball glove\n",
    "37. skateboard\n",
    "38. surfboard\n",
    "39. tennis racket\n",
    "40. bottle\n",
    "41. wine glass\n",
    "42. cup\n",
    "43. fork\n",
    "44. knife\n",
    "45. spoon\n",
    "46. bowl\n",
    "47. banana\n",
    "48. apple\n",
    "49. sandwich\n",
    "50. orange\n",
    "51. broccoli\n",
    "52. carrot\n",
    "53. hot dog\n",
    "54. pizza\n",
    "55. donut\n",
    "56. cake\n",
    "57. chair\n",
    "58. couch\n",
    "59. potted plant\n",
    "60. bed\n",
    "61. dining table\n",
    "62. toilet\n",
    "63. tv\n",
    "64. laptop\n",
    "65. mouse\n",
    "66. remote\n",
    "67. keyboard\n",
    "68. cell phone\n",
    "69. microwave\n",
    "70. oven\n",
    "71. toaster\n",
    "72. sink\n",
    "73. refrigerator\n",
    "74. book\n",
    "75. clock\n",
    "76. vase\n",
    "77. scissors\n",
    "78. teddy bear\n",
    "79. hair drier\n",
    "80. toothbrush\n",
    "\n",
    "4. Click the icon Extract prediction as shown below:\n",
    "\n",
    "![Puppy](./img/extract.png)\n",
    "\n",
    "You can see the output of the prediction on the basis of the uploaded image.\n",
    "\n",
    "![Puppy2](./img/predict.png)\n",
    "\n",
    "Here the background and the dog image are separated, showing two different parts of the image.\n",
    "\n",
    "You can also try the webcam option, which will show the real-time prediction by the toggle-on webcam option.\n",
    "\n",
    "This concludes Exercise 2 of this lab, which introduced the Model Asset Exchange (MAX).\n",
    "\n",
    "You can also watch a demo of the object detector model [here](https://video.ibm.com/recorded/128825527?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMDeveloperSkillsNetwork-DS0105EN-SkillsNetwork).\n",
    "\n",
    "Author(s)\n",
    "Joseph Santarcangelo\n",
    "Other Contributor(s)\n",
    "Lavanya"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
